{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Runnable Primitives\n",
        "# RunnableSequence"
      ],
      "metadata": {
        "id": "8gPVf2NdI0qC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j96N7dvnGjn5",
        "outputId": "b372af81-5bff-4c69-a3c7-92fd703b7307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m2.2/2.5 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain-community google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M4p-80-MJU9G",
        "outputId": "8873c783-9def-4257-fef2-6a45880bb15d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.68)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.4.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.14.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.7-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain-google-genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain-google-genai-2.1.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "f59b8e41a30747ecb46fedad1ba877bf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Access the API key from Colab's Secrets Manager\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "WYVNmXYhJXwR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.5)"
      ],
      "metadata": {
        "id": "qXhJVF4IJatB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableSequence\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template=\"write a joke about {topic}\",\n",
        "    input_variables=[\"topic\"]\n",
        ")\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# RunnableSequence(prompt1 | model | parser) works too\n",
        "\n",
        "chain = RunnableSequence(\n",
        "    prompt1 , model , parser\n",
        ")\n",
        "\n",
        "result = chain.invoke({\"topic\":\"science\"})\n",
        "display(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fwUCm-GHJeit",
        "outputId": "99f4add2-c997-4425-8c30-efafad81a8b2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grandalf\n",
        "chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU3s7JmOMKsB",
        "outputId": "c9f51ac9-b1ee-4fc1-c3ce-b8163c10c2a9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting grandalf\n",
            "  Downloading grandalf-0.8-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from grandalf) (3.2.3)\n",
            "Downloading grandalf-0.8-py3-none-any.whl (41 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: grandalf\n",
            "Successfully installed grandalf-0.8\n",
            "      +-------------+      \n",
            "      | PromptInput |      \n",
            "      +-------------+      \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "    +----------------+     \n",
            "    | PromptTemplate |     \n",
            "    +----------------+     \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "+------------------------+ \n",
            "| ChatGoogleGenerativeAI | \n",
            "+------------------------+ \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "    +-----------------+    \n",
            "    | StrOutputParser |    \n",
            "    +-----------------+    \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "+-----------------------+  \n",
            "| StrOutputParserOutput |  \n",
            "+-----------------------+  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableSequence\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template=\"write a joke about {topic}\",\n",
        "    input_variables=[\"topic\"]\n",
        ")\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template=\"explain the following joke-{text}\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "chain = RunnableSequence(\n",
        "    prompt1, model, parser, prompt2, model, parser\n",
        ")\n",
        "\n",
        "result = chain.invoke({\"topic\":\"AI\"})\n",
        "display(result)"
      ],
      "metadata": {
        "id": "nKxAN4rWLubQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RunnableParallel\n",
        "already done in chains"
      ],
      "metadata": {
        "id": "h8klV-_wLkUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableSequence\n",
        "\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template=\"Give one line tweet about {topic}\",\n",
        "    input_variables=[\"topic\"]\n",
        ")\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template=\"generate one line linkedin post about {topic}\",\n",
        "    input_variables=[\"topic\"]\n",
        ")\n",
        "\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.5)\n",
        "\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "\n",
        "parallel_chain = RunnableParallel({\n",
        "    \"tweet\": RunnableSequence(\n",
        "        prompt1, model, parser\n",
        "    ),\n",
        "    \"linkedin\":RunnableSequence(\n",
        "        prompt2, model, parser\n",
        "    )\n",
        "    })\n",
        "\n",
        "result = parallel_chain.invoke({\"topic\":\"AI\"})\n",
        "\n",
        "display(result)"
      ],
      "metadata": {
        "id": "oqz6l8CsL3iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RunnablePassthrough\n",
        "give same output as input"
      ],
      "metadata": {
        "id": "QZl-EoBaPJ0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableSequence, RunnablePassthrough\n",
        "\n",
        "passthrough = RunnablePassthrough()\n",
        "\n",
        "print(passthrough.invoke(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwZNrKAYPcAw",
        "outputId": "c293dc53-0894-4e1e-db0b-4ed54d845729"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableSequence, RunnablePassthrough\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template=\"write a joke about {topic}\",\n",
        "    input_variables=[\"topic\"]\n",
        ")\n",
        "\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.5)\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template=\"explain the following joke-{text}\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "\n",
        "jokes_generator_chain = RunnableSequence(\n",
        "    prompt1, model, parser\n",
        ")\n",
        "\n",
        "parallel_chain = RunnableParallel({\n",
        "    \"joke\": RunnablePassthrough(),\n",
        "    \"explanation\":RunnableSequence(\n",
        "        prompt2, model , parser\n",
        "    )\n",
        "})\n",
        "\n",
        "\n",
        "final_chain = RunnableSequence(jokes_generator_chain, parallel_chain)\n",
        "\n",
        "final_chain.invoke({\"topic\":\"cricket\"})"
      ],
      "metadata": {
        "id": "bCL2UWskSMNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RunnableLambda: convert python function to runnable functions\n",
        "eg. uses to preprocess the data : add custom logic to the runnable"
      ],
      "metadata": {
        "id": "mD6_1Tr3VdlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableLambda\n",
        "\n",
        "def words_counter(text):\n",
        "  return len(text.split())\n",
        "\n",
        "\n",
        "\n",
        "runnable_word_counter = RunnableLambda(words_counter)\n",
        "\n",
        "print(runnable_word_counter.invoke(\"this is awesome\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCHQZiKhVcz_",
        "outputId": "0ffd7502-6190-4af0-efd9-3f5f272f5ffd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableSequence, RunnablePassthrough, RunnableLambda\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"write a joke about {topic}\",\n",
        "    input_variables=[\"topic\"]\n",
        ")\n",
        "\n",
        "def words_counter(text):\n",
        "  return len(text.split())\n",
        "\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.5)\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "joke_generator_chain = RunnableSequence(prompt, model , parser)\n",
        "\n",
        "parallel_chain = RunnableParallel({\n",
        "    \"joke\":RunnablePassthrough(),\n",
        "    # \"word_count\":RunnableLambda(words_counter)\n",
        "    \"word_count\":RunnableLambda(lambda x: len(x.split()))\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "final_chain = RunnableSequence(joke_generator_chain, parallel_chain)\n",
        "\n",
        "print(final_chain.invoke({\"topic\":\"AI\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4WSUP-9VZ3l",
        "outputId": "f4d26b57-df47-4f45-a538-3ebe109dd832"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'joke': \"Here are a few options, pick your favorite!\\n\\n**Option 1 (Classic AI trope):**\\n> Why did the AI break up with its calculator?\\n> Because it said they just didn't *compute* their feelings.\\n\\n**Option 2 (Current AI issue):**\\n> My AI told me a joke the other day.\\n> It was hilarious, but then I fact-checked it and found out the punchline was completely made up.\\n\\n**Option 3 (Relatable job market humor):**\\n> My boss replaced me with an AI.\\n> Now I just sit around all day, prompting it to write my resume.\\n\\n**Option 4 (Literal interpretation):**\\n> Why did the AI cross the road?\\n> To optimize its route efficiency, but it still couldn't tell you *why* the chicken did.\", 'word_count': 122}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "you are getting more jokes because of the term temperature = 0.5 so decrease it to 0 or remove it"
      ],
      "metadata": {
        "id": "ce1o2ttBanGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RunnableBranch\n",
        "if else statement of langchain"
      ],
      "metadata": {
        "id": "hh_cr1htam_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableSequence, RunnablePassthrough, RunnableLambda, RunnableBranch\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template=\"write a simple report on {topic}\",\n",
        "    input_variables=[\"topic\"]\n",
        ")\n",
        "\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template=\"summarize the following {text}\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "\n",
        "report_gen_chain = RunnableSequence(prompt1 , model , parser)\n",
        "\n",
        "branch_chain = RunnableBranch(\n",
        "    (lambda x: len(x.split()) > 500, RunnableSequence(prompt2, model, parser)),\n",
        "    (RunnablePassthrough())\n",
        ")\n",
        "\n",
        "\n",
        "final_chain = RunnableSequence(report_gen_chain | branch_chain)\n",
        "\n",
        "final_chain.invoke({\"topic\":\"Influence of AI\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "6sL4MFFCalZr",
        "outputId": "2da1004d-b5ee-4d63-85eb-cc4cd056add6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"This report outlines the profound and pervasive influence of Artificial Intelligence (AI) across various sectors.\\n\\n**Key areas of AI's influence include:**\\n\\n*   **Work and Economy:** Driving automation for efficiency, transforming job roles by displacing some while creating new ones, and fostering economic growth through innovation.\\n*   **Daily Life:** Powering smart devices, personalizing recommendations, and enhancing navigation and future transportation.\\n*   **Healthcare:** Assisting in faster diagnosis, drug discovery, personalized treatments, and improving operational efficiency in medical facilities.\\n*   **Creativity and Education:** Generating diverse content (text, images, music) and enabling personalized learning experiences for students.\\n*   **Research and Development:** Accelerating scientific discovery through vast data analysis and complex system simulations.\\n\\n**However, the report also highlights significant challenges and considerations:**\\n\\n*   **Job Displacement:** Concerns about potential widespread job losses due to automation.\\n*   **Ethical Concerns:** Issues like algorithmic bias, privacy violations, and the potential for misuse (e.g., autonomous weapons, deepfakes).\\n*   **Security Risks:** Vulnerabilities of AI systems to cyberattacks.\\n*   **Digital Divide:** The risk of exacerbating inequalities due to unequal access to AI technologies.\\n\\nIn conclusion, AI is a transformative technology offering immense benefits in efficiency, personalization, and innovation. The report stresses the critical need for a responsible and ethical approach to AI development to ensure its advantages benefit all of humanity while mitigating potential risks.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hc9wv7stg08s",
        "outputId": "57e1f7f4-d1a5-437d-d293-e2a667de51d7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      +-------------+      \n",
            "      | PromptInput |      \n",
            "      +-------------+      \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "    +----------------+     \n",
            "    | PromptTemplate |     \n",
            "    +----------------+     \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "+------------------------+ \n",
            "| ChatGoogleGenerativeAI | \n",
            "+------------------------+ \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "    +-----------------+    \n",
            "    | StrOutputParser |    \n",
            "    +-----------------+    \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "        +--------+         \n",
            "        | Branch |         \n",
            "        +--------+         \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "     +--------------+      \n",
            "     | BranchOutput |      \n",
            "     +--------------+      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LCEL\n",
        "RunnableSequence is most common runnable so replace\n",
        "\n",
        "RunnableSequence(r1 , r2, r3) ------> r1 | r2 | r3 use this syntax instead\n",
        "pipe operator"
      ],
      "metadata": {
        "id": "UzvJMNX8hOd_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8AbmB-5gg_F3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}