{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvjaTAh4jT0M",
        "outputId": "0838148f-727f-4332-e672-de45a6d3242e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain-community google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oSzdGnpmjXKn",
        "outputId": "5a0d34a5-cf3a-4d1c-bc2e-0dd471a33655"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.68)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.4.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.14.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.7-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain-google-genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain-google-genai-2.1.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "ad8d785ece794ceba5c1bbaf72172f2d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Access the API key from Colab's Secrets Manager\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "8bI0RcQKjeGz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple chain"
      ],
      "metadata": {
        "id": "7HrudOuC0y5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.5)"
      ],
      "metadata": {
        "id": "BLdFunl_julu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "mWsjFdbCzpbf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "# build the prompt : step1\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"generate 2 intersting facts about {topic}\",\n",
        "    input_variables=[\"topic\"]\n",
        ")\n",
        "\n",
        "# build the model: step2\n",
        "\n",
        "\n",
        "# use parser for result generation: step3\n",
        "parser = StrOutputParser()\n",
        "\n",
        "\n",
        "# connect all of this steps using the | to form chain...\n",
        "chain = prompt | model | parser\n",
        "\n",
        "result = chain.invoke({\"topic\":\"football\"})\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqDwaQ5rkA4C",
        "outputId": "f0978cef-c894-4a7e-e454-d88384768f28"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are 2 interesting facts about football (soccer):\n",
            "\n",
            "1.  **The Offside Rule Was Once Much Stricter:** In the early days of football, a player was considered offside if they were ahead of the ball at all, similar to rugby. The modern offside rule, which requires at least two opponents (including the goalkeeper) to be between an attacking player and the goal line when the ball is played, was gradually introduced. A significant change in 1925 reduced the number of opponents needed from three to two, which drastically increased goal-scoring and fundamentally changed the attacking dynamics of the game.\n",
            "\n",
            "2.  **A Goalkeeper Holds the Record for Most Goals Scored by a Goalkeeper:** While goalkeepers are primarily known for preventing goals, Brazilian legend Rog√©rio Ceni holds the astonishing record for the most goals scored by a goalkeeper in professional football. During his career with S√£o Paulo FC, he netted an incredible 131 goals, primarily from free-kicks and penalties, making him a unique attacking threat from between the posts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grandalf\n",
        "chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ7v4va8zPzk",
        "outputId": "28cc05b9-2038-4c9f-9774-21d3cf151e05"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting grandalf\n",
            "  Downloading grandalf-0.8-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from grandalf) (3.2.3)\n",
            "Downloading grandalf-0.8-py3-none-any.whl (41 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/41.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: grandalf\n",
            "Successfully installed grandalf-0.8\n",
            "      +-------------+      \n",
            "      | PromptInput |      \n",
            "      +-------------+      \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "    +----------------+     \n",
            "    | PromptTemplate |     \n",
            "    +----------------+     \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "+------------------------+ \n",
            "| ChatGoogleGenerativeAI | \n",
            "+------------------------+ \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "    +-----------------+    \n",
            "    | StrOutputParser |    \n",
            "    +-----------------+    \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "+-----------------------+  \n",
            "| StrOutputParserOutput |  \n",
            "+-----------------------+  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequential chain"
      ],
      "metadata": {
        "id": "K78R8D3z0v1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template=\"generate a simple short report on {topic}\",\n",
        "    input_variables=[\"topic\"]\n",
        ")\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template=\"generate 2 points summary for the following text \\n {text}\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "# model\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "\n",
        "chain = prompt1 | model | parser | prompt2 | model | parser\n",
        "\n",
        "result = chain.invoke({\"topic\":\"umemployment in Nepal\"})\n",
        "\n",
        "display(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "STs1Xy_vz5Ut",
        "outputId": "a462d38f-9212-49e7-9610-8542db08e3ca"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Here are two points summarizing the text:\\n\\n1.  Unemployment and underemployment are major socio-economic challenges in Nepal, leading to a \"brain drain\" of skilled youth seeking jobs abroad, economic hardship, and heavy reliance on remittances.\\n2.  The problem is driven by limited job creation, a mismatch between skills and market demands, and slow industrial growth, necessitating a multi-faceted approach including vocational training, entrepreneurship promotion, and education reform.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3zPQuHQ2OeQ",
        "outputId": "6560ac2b-e1e2-4b21-9545-53bf33c7c597"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      +-------------+      \n",
            "      | PromptInput |      \n",
            "      +-------------+      \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "    +----------------+     \n",
            "    | PromptTemplate |     \n",
            "    +----------------+     \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "+------------------------+ \n",
            "| ChatGoogleGenerativeAI | \n",
            "+------------------------+ \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "    +-----------------+    \n",
            "    | StrOutputParser |    \n",
            "    +-----------------+    \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "+-----------------------+  \n",
            "| StrOutputParserOutput |  \n",
            "+-----------------------+  \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "    +----------------+     \n",
            "    | PromptTemplate |     \n",
            "    +----------------+     \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "+------------------------+ \n",
            "| ChatGoogleGenerativeAI | \n",
            "+------------------------+ \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "    +-----------------+    \n",
            "    | StrOutputParser |    \n",
            "    +-----------------+    \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "+-----------------------+  \n",
            "| StrOutputParserOutput |  \n",
            "+-----------------------+  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parallel chain :\n",
        "user provides a document and generate notes and quiz and combines them and show to user.\n",
        "docs----first model -----> notes --------|\n",
        " |same - docs______sec model ------->quiz ----------|(combined both o/p)\n",
        "                                         |----model 3-----> output"
      ],
      "metadata": {
        "id": "2p-f5OOY2uYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q mistralai\n",
        "%pip install -q langchain-mistralai\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel # to execute multiple parallel chain\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "model1 = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.5)\n",
        "\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "mistral_model = ChatMistralAI(model=\"mistral-small\")\n",
        "\n",
        "\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template=\"generate a simple very short notes on {text}\",\n",
        "    input_variables=[\"text\"]\n",
        "\n",
        ")\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template=\"genarate a short question answer for the following text \\n {text}\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "prompt3 = PromptTemplate(\n",
        "    template=\"merge the provided notes and quiz into single document \\n notes-> {notes} \\n quiz -> {quiz}\",\n",
        "    input_variables=[\"notes\",\"quiz\"]\n",
        ")\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "parallel_chain = RunnableParallel(\n",
        "    {\n",
        "\n",
        "     \"notes\": prompt1 | model1 | parser,\n",
        "     \"quiz\":prompt2 | mistral_model | parser\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "merge_chain = prompt3 | mistral_model | parser\n",
        "\n",
        "chain = parallel_chain | merge_chain\n",
        "\n",
        "text = \"\"\"ONNX, which stands for Open Neural Network Exchange, is an open standard format for representing machine learning models.\n",
        "It allows models trained in one framework (like PyTorch or TensorFlow) to be easily exported and used in other frameworks or environments without significant modifications.\"\"\"\n",
        "result = chain.invoke({\"text\":text})\n",
        "\n",
        "display(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "zBte5HGD9E_y",
        "outputId": "d9eafdab-3692-44a6-d4ad-0cb3845f5e8f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"ONNX (Open Neural Network Exchange) Notes and Quiz:\\n\\nONNX is an open standard format for representing machine learning models. It enables models trained in one framework, such as PyTorch or TensorFlow, to be easily exported and used in other environments. This cross-framework compatibility is crucial in ensuring that machine learning models can be shared and utilized across different platforms and tools.\\n\\nQuiz:\\n\\nQ: What does ONNX stand for?\\nA: ONNX stands for Open Neural Network Exchange. It's an open standard format for representing machine learning models, allowing models trained in one framework to be exported and used in other frameworks without significant modifications.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw9K15GGDI99",
        "outputId": "7789133c-3bb2-4939-e5ff-3d75ff47e71e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  +---------------------------+                \n",
            "                  | Parallel<notes,quiz>Input |                \n",
            "                  +---------------------------+                \n",
            "                     ****               ****                   \n",
            "                  ***                       ***                \n",
            "                **                             **              \n",
            "    +----------------+                    +----------------+   \n",
            "    | PromptTemplate |                    | PromptTemplate |   \n",
            "    +----------------+                    +----------------+   \n",
            "             *                                      *          \n",
            "             *                                      *          \n",
            "             *                                      *          \n",
            "+------------------------+                 +---------------+   \n",
            "| ChatGoogleGenerativeAI |                 | ChatMistralAI |   \n",
            "+------------------------+                 +---------------+   \n",
            "             *                                      *          \n",
            "             *                                      *          \n",
            "             *                                      *          \n",
            "    +-----------------+                   +-----------------+  \n",
            "    | StrOutputParser |                   | StrOutputParser |  \n",
            "    +-----------------+                   +-----------------+  \n",
            "                     ****               ****                   \n",
            "                         ***         ***                       \n",
            "                            **     **                          \n",
            "                 +----------------------------+                \n",
            "                 | Parallel<notes,quiz>Output |                \n",
            "                 +----------------------------+                \n",
            "                                *                              \n",
            "                                *                              \n",
            "                                *                              \n",
            "                       +----------------+                      \n",
            "                       | PromptTemplate |                      \n",
            "                       +----------------+                      \n",
            "                                *                              \n",
            "                                *                              \n",
            "                                *                              \n",
            "                        +---------------+                      \n",
            "                        | ChatMistralAI |                      \n",
            "                        +---------------+                      \n",
            "                                *                              \n",
            "                                *                              \n",
            "                                *                              \n",
            "                       +-----------------+                     \n",
            "                       | StrOutputParser |                     \n",
            "                       +-----------------+                     \n",
            "                                *                              \n",
            "                                *                              \n",
            "                                *                              \n",
            "                    +-----------------------+                  \n",
            "                    | StrOutputParserOutput |                  \n",
            "                    +-----------------------+                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# setup for the mistral AI is done here üòä"
      ],
      "metadata": {
        "id": "m4yheU_dC0dd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0da934d"
      },
      "source": [
        "%pip install -q mistralai"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f026eff"
      },
      "source": [
        "Now, let's set up your Mistral API key. We'll use Colab's Secrets Manager for secure storage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdd0b134"
      },
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"MISTRAL_API_KEY\"] = userdata.get(\"MISTRAL_API_KEY\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da45f9e6"
      },
      "source": [
        "Finally, let's initialize the Mistral model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03f9ac60"
      },
      "source": [
        "from langchain_mistralai import ChatMistralAI\n",
        "\n",
        "mistral_model = ChatMistralAI(model=\"mistral-small\") # You can change the model name if needed"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef0eab95"
      },
      "source": [
        "%pip install -q langchain-mistralai"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "# build the prompt : step1\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"generate 2 intersting facts about {topic}\",\n",
        "    input_variables=[\"topic\"]\n",
        ")\n",
        "\n",
        "# build the model: step2\n",
        "\n",
        "\n",
        "# use parser for result generation: step3\n",
        "parser = StrOutputParser()\n",
        "\n",
        "\n",
        "# connect all of this steps using the | to form chain...\n",
        "chain = prompt | mistral_model | parser\n",
        "\n",
        "result = chain.invoke({\"topic\":\"football\"})\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PC05HYICCXK",
        "outputId": "04c83d52-7121-464b-ffce-a9c4179a4034"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Football, or soccer as it's known in some countries, is the most popular sport in the world. According to FIFA, the sport's international governing body, there are more than 240 million players actively involved in football around the globe.\n",
            "\n",
            "2. The longest football match in history took place in 1986 in Buenos Aires, Argentina between two amateur teams, San Lorenzo de Almagro and Hurac√°n de Montemar. The match ended in a 2-1 victory for San Lorenzo after a whopping 23 hours, 14 minutes of play. The match was played over four days with several intermissions for players to rest and refuel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conditional chains : give feedback according to the feedback after analyzing the sentiment"
      ],
      "metadata": {
        "id": "1FCwXZspEQE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel , RunnableBranch, RunnableLambda\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from typing import Literal\n",
        "\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "mistral_model = ChatMistralAI(model=\"mistral-small\")\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "\n",
        "class Feedback(BaseModel):\n",
        "\n",
        "  sentiment: Literal['positive', 'negative'] = Field(description=\"give the sentiment of the parser\")\n",
        "\n",
        "\n",
        "parser2 = PydanticOutputParser(pydantic_object=Feedback)\n",
        "\n",
        "\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template=\"classify the sentiment of the following feedback text into 'positive' or 'negative' only: \\n {feedback} \\n {format_instructions}\",\n",
        "    input_variables=['feedback'],\n",
        "    partial_variables={\"format_instructions\":parser2.get_format_instructions()}\n",
        ")\n",
        "\n",
        "\n",
        "classifier_chain = prompt1 | mistral_model | parser2\n",
        "\n",
        "# result = classifier_chain.invoke({\"feedback\":\"The food was delicious\"}).sentiment\n",
        "# print(result)\n",
        "# sentiment = 'positive' <-- output\n",
        "\n",
        "# write prompt for each branch\n",
        "\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template=\"write an appropriate response to this positive feedback \\n {feedback}\",\n",
        "    input_variables=[\"feedback\"]\n",
        ")\n",
        "\n",
        "prompt3 = PromptTemplate(\n",
        "    template=\"write an appropriate response to this negative feedback \\n {feedback}\",\n",
        "    input_variables=[\"feedback\"]\n",
        ")\n",
        "\n",
        "branch_chain = RunnableBranch(\n",
        "     (lambda x: x.sentiment == 'positive', prompt2 | mistral_model | parser),\n",
        "    (lambda x: x.sentiment == 'negative', prompt3 | mistral_model |parser),\n",
        "    RunnableLambda( lambda x:\"could not find sentiment\")\n",
        ")\n",
        "\n",
        "\n",
        "chain = classifier_chain | branch_chain\n",
        "\n",
        "result = chain.invoke({\"feedback\":\" this is a terrible phone\"})\n",
        "display(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "0v96NLtsCUF7",
        "outputId": "262cb3c1-fa2a-4d5f-8fbb-7c965441b54e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Dear Customer,\\n\\nWe are sorry to hear that your experience with us was not a positive one. Our goal is to provide excellent service and products to all of our customers, and it seems that we fell short during your interaction with us.\\n\\nWe appreciate your feedback and would like to take the opportunity to make things right. Please contact us directly so that we can discuss your concerns and work towards a resolution. Our customer service team is available to assist you and can be reached at [insert contact information].\\n\\nOnce again, we apologize for any inconvenience and thank you for bringing this matter to our attention. We value your business and hope to have the opportunity to serve you better in the future.\\n\\nBest regards,\\n[Your Name]\\n[Your Position]\\n[Company Name]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXX53UQhGdlo",
        "outputId": "daf28a33-ad88-40c7-98f4-94bb63ffcad6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    +-------------+      \n",
            "    | PromptInput |      \n",
            "    +-------------+      \n",
            "            *            \n",
            "            *            \n",
            "            *            \n",
            "   +----------------+    \n",
            "   | PromptTemplate |    \n",
            "   +----------------+    \n",
            "            *            \n",
            "            *            \n",
            "            *            \n",
            "    +---------------+    \n",
            "    | ChatMistralAI |    \n",
            "    +---------------+    \n",
            "            *            \n",
            "            *            \n",
            "            *            \n",
            "+----------------------+ \n",
            "| PydanticOutputParser | \n",
            "+----------------------+ \n",
            "            *            \n",
            "            *            \n",
            "            *            \n",
            "       +--------+        \n",
            "       | Branch |        \n",
            "       +--------+        \n",
            "            *            \n",
            "            *            \n",
            "            *            \n",
            "    +--------------+     \n",
            "    | BranchOutput |     \n",
            "    +--------------+     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n3zoNYZCNKma"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}